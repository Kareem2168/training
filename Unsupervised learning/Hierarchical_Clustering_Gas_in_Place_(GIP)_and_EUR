import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
files.upload()

df=pd.read_excel('Chapter4_ GIP_EUR_DataSet.xlsx')
df.head()

df.describe()

#in this case, we should standardized our data inorder to have accurate results (since there is large difference in numbers of both columns compared)


from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
df_scaled=scaler.fit_transform(df)


"ward" minimizes the variance of the
clusters being merged. 

"average" uses the average of the distances of each
observation of the two sets. 

"complete" or "maximum" linkage uses the
maximum distances between all observations of the two sets. 

"single" uses the
minimum of the distances between all observations of the two sets. The default
linkage of "ward" was used in this example.

#Like other clustering methods, Ward's method starts with n clusters, each containing a single object. These n clusters are combined to make one cluster containing all objects. At each step, the process makes a new cluster that minimizes variance, measured by an index called E (also called the sum of squares index)

#here we need to use dendrogram to determine clusters needed 
plt.figure(figsize=(15, 8))
import scipy.cluster.hierarchy as shc 
dend=shc.dendrogram(shc.linkage(df_scaled, method='ward'))
plt.title('dendrogram')
plt.xlabel('samples')
plt.ylabel('distance')

# now we apply agglomerative clustering

from sklearn.cluster import AgglomerativeClustering 
HC=AgglomerativeClustering(5)
HC=HC.fit_predict(df_scaled)
HC

#convert the scaled df to dataframe
#add a new column with HC

df_new=pd.DataFrame(df_scaled, columns=['GIP(BCFperSection)', 'EUR/1000ft'])
df_new['clusters']=HC
df_new

#using K Means 
from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer
fig, ax=plt.subplots(2,2)
for i in [2,3,4,5]:
  km=KMeans(i)
  q,mod=divmod(i,2)
  visualizer=SilhouetteVisualizer(km,ax=ax[q-1,mod], colors='yellowbrick')
  ax[q-1,mod].set_title(i)
  visualizer.fit(df_new)

plt.tight_layout()

# see the score for 1 cluster 
from sklearn.metrics import silhouette_samples, silhouette_score
km=KMeans(5)
km.fit_predict(df_new)
score=silhouette_score(df_new,km.labels_, metric='euclidean')
score


#################################################

#Silhouette method to get the score
#for visualizing the whole 5 clusters we use the following 



cluster_labels=np.unique(HC)
silhouette_vals=silhouette_samples(df_new, HC)
ylow=yupp=0
yticks=[]
for i, c in enumerate(cluster_labels):
  c_silh_vals=silhouette_vals[HC==c]
  c_silh_vals.sort()
  
  yupp+=len(c_silh_vals)
  plt.barh(range(ylow,yupp), c_silh_vals, height=1)
  yticks.append((ylow+yupp)/2)
  ylow+=len(c_silh_vals)

silhouette_avg=np.mean(silhouette_vals)
plt.axvline(silhouette_avg, color='blue', linestyle='--')
plt.title('Silhouette coefficient after applying hierarchical clustering.')
plt.xlabel('Silhouette coeffecient')
plt.yticks(yticks,cluster_labels+1)
plt.ylabel('clusters')

#assign centroids to clusters
KM=km.cluster_centers_
sns.scatterplot(x=df_new['EUR/1000ft'],y=df_new['GIP(BCFperSection)'], hue=df_new['clusters'])
plt.scatter(KM[:, 0], KM[:, 1], marker='+', color='red', linewidth=4)
plt.legend()

#to see each each cluster and its mean values 
df_new.groupby(by='clusters').count()

